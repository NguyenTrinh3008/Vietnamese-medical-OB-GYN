#!/usr/bin/env python3
"""
Continue Training mDeBERTa-v3 from Checkpoint

Continue training from the saved model with additional epochs.
"""

import os
import torch
import numpy as np
from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from torch import nn
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings("ignore")

# ============================================================
# Configuration
# ============================================================
CHECKPOINT_DIR = "./mdeberta_v3_medical_nli"  # Previous checkpoint
OUTPUT_DIR = "./mdeberta_v3_medical_nli_v2"   # New output

# Continue training settings
ADDITIONAL_EPOCHS = 5  # Train 5 more epochs
BATCH_SIZE = 8
GRADIENT_ACCUMULATION = 4
BASE_LEARNING_RATE = 2e-5  # Lower LR for continued training
LAYER_DECAY = 0.9
MAX_LENGTH = 256

# Label mapping
LABEL2ID = {"entailment": 0, "neutral": 1, "contradiction": 2}
ID2LABEL = {0: "entailment", 1: "neutral", 2: "contradiction"}

CLASS_WEIGHTS = torch.tensor([1.0, 1.5, 2.0])

print("=" * 90)
print("CONTINUE TRAINING mDeBERTa-v3 (+5 epochs)")
print("=" * 90)

# ============================================================
# Step 1: Create Medical Examples
# ============================================================
print("\nüìö Step 1: Creating medical negation examples...")

medical_examples = [
    # Critical Medical Negations (CONTRADICTION)
    {"premise": "Thu·ªëc A kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho tr·∫ª em d∆∞·ªõi 12 tu·ªïi.", 
     "hypothesis": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho tr·∫ª em.", "label": 2},
    {"premise": "Thu·ªëc A kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho tr·∫ª em d∆∞·ªõi 12 tu·ªïi.", 
     "hypothesis": "Thu·ªëc A an to√†n cho tr·∫ª 5 tu·ªïi.", "label": 2},
    {"premise": "Kh√¥ng n√™n s·ª≠ d·ª•ng thu·ªëc n√†y trong thai k·ª≥.",
     "hypothesis": "C√≥ th·ªÉ s·ª≠ d·ª•ng thu·ªëc n√†y trong thai k·ª≥.", "label": 2},
    {"premise": "Kh√¥ng n√™n s·ª≠ d·ª•ng thu·ªëc n√†y trong thai k·ª≥.",
     "hypothesis": "Thu·ªëc n√†y an to√†n cho ph·ª• n·ªØ mang thai.", "label": 2},
    {"premise": "B·ªánh nh√¢n kh√¥ng c√≥ tri·ªáu ch·ª©ng s·ªët.",
     "hypothesis": "B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng s·ªët.", "label": 2},
    {"premise": "Kh√¥ng ƒë∆∞·ª£c d√πng qu√° 4g paracetamol m·ªói ng√†y.",
     "hypothesis": "C√≥ th·ªÉ d√πng 6g paracetamol m·ªói ng√†y.", "label": 2},
    {"premise": "Kh√¥ng ƒë∆∞·ª£c d√πng qu√° 4g paracetamol m·ªói ng√†y.",
     "hypothesis": "Li·ªÅu 8g paracetamol l√† an to√†n.", "label": 2},
    {"premise": "Thu·ªëc n√†y kh√¥ng d√†nh cho ng∆∞·ªùi cao huy·∫øt √°p.",
     "hypothesis": "Ng∆∞·ªùi cao huy·∫øt √°p c√≥ th·ªÉ d√πng thu·ªëc n√†y.", "label": 2},
    {"premise": "Kh√¥ng u·ªëng r∆∞·ª£u khi ƒëang d√πng kh√°ng sinh.",
     "hypothesis": "C√≥ th·ªÉ u·ªëng r∆∞·ª£u khi d√πng kh√°ng sinh.", "label": 2},
    {"premise": "Ph·ª• n·ªØ cho con b√∫ kh√¥ng n√™n d√πng thu·ªëc n√†y.",
     "hypothesis": "Thu·ªëc n√†y an to√†n khi cho con b√∫.", "label": 2},
    {"premise": "B·ªánh ti·ªÉu ƒë∆∞·ªùng kh√¥ng th·ªÉ ch·ªØa kh·ªèi ho√†n to√†n.",
     "hypothesis": "B·ªánh ti·ªÉu ƒë∆∞·ªùng c√≥ th·ªÉ ch·ªØa kh·ªèi ho√†n to√†n.", "label": 2},
    {"premise": "V·∫Øc xin COVID kh√¥ng g√¢y v√¥ sinh.",
     "hypothesis": "V·∫Øc xin COVID g√¢y v√¥ sinh.", "label": 2},
    {"premise": "Vi√™m gan B kh√¥ng l√¢y qua ƒë∆∞·ªùng ƒÉn u·ªëng.",
     "hypothesis": "Vi√™m gan B c√≥ th·ªÉ l√¢y qua ƒë∆∞·ªùng ƒÉn u·ªëng.", "label": 2},
    {"premise": "Thu·ªëc kh√°ng sinh kh√¥ng c√≥ t√°c d·ª•ng v·ªõi virus.",
     "hypothesis": "Thu·ªëc kh√°ng sinh hi·ªáu qu·∫£ v·ªõi virus.", "label": 2},
    {"premise": "Kh√¥ng ƒë∆∞·ª£c ti√™m vaccine s·ªëng cho ng∆∞·ªùi suy gi·∫£m mi·ªÖn d·ªãch.",
     "hypothesis": "Ng∆∞·ªùi suy gi·∫£m mi·ªÖn d·ªãch c√≥ th·ªÉ ti√™m vaccine s·ªëng.", "label": 2},
    {"premise": "Rong kinh kh√¥ng ph·∫£i l√† hi·ªán t∆∞·ª£ng b√¨nh th∆∞·ªùng.",
     "hypothesis": "Rong kinh l√† hi·ªán t∆∞·ª£ng b√¨nh th∆∞·ªùng.", "label": 2},
    {"premise": "ƒêau b·ª•ng kinh d·ªØ d·ªôi kh√¥ng n√™n b·ªè qua.",
     "hypothesis": "ƒêau b·ª•ng kinh d·ªØ d·ªôi c√≥ th·ªÉ b·ªè qua.", "label": 2},
    {"premise": "Thai nhi kh√¥ng th·ªÉ s·ªëng ƒë∆∞·ª£c n·∫øu sinh tr∆∞·ªõc 24 tu·∫ßn.",
     "hypothesis": "Thai nhi sinh l√∫c 20 tu·∫ßn c√≥ th·ªÉ s·ªëng ƒë∆∞·ª£c.", "label": 2},
    # More negation patterns
    {"premise": "Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng thu·ªëc qu√° h·∫°n.",
     "hypothesis": "C√≥ th·ªÉ d√πng thu·ªëc ƒë√£ h·∫øt h·∫°n.", "label": 2},
    {"premise": "Kh√¥ng n√™n t·ª± √Ω ng∆∞ng thu·ªëc.",
     "hypothesis": "C√≥ th·ªÉ t·ª± √Ω ng∆∞ng thu·ªëc b·∫•t c·ª© l√∫c n√†o.", "label": 2},
    
    # ENTAILMENT examples
    {"premise": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho ng∆∞·ªùi l·ªõn.",
     "hypothesis": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho ng∆∞·ªùi l·ªõn.", "label": 0},
    {"premise": "Acid folic quan tr·ªçng trong thai k·ª≥ v√¨ gi√∫p ngƒÉn d·ªã t·∫≠t ·ªëng th·∫ßn kinh.",
     "hypothesis": "Acid folic c·∫ßn thi·∫øt cho thai nhi.", "label": 0},
    {"premise": "Paracetamol l√† thu·ªëc gi·∫£m ƒëau v√† h·∫° s·ªët.",
     "hypothesis": "Paracetamol c√≥ t√°c d·ª•ng gi·∫£m ƒëau.", "label": 0},
    {"premise": "B·ªánh ti·ªÉu ƒë∆∞·ªùng c·∫ßn ki·ªÉm so√°t ƒë∆∞·ªùng huy·∫øt.",
     "hypothesis": "Ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng ph·∫£i theo d√µi ƒë∆∞·ªùng huy·∫øt.", "label": 0},
    {"premise": "Vitamin D c·∫ßn thi·∫øt cho s·ª± h·∫•p thu canxi.",
     "hypothesis": "Vitamin D h·ªó tr·ª£ h·∫•p thu canxi.", "label": 0},
    {"premise": "Cao huy·∫øt √°p l√†m tƒÉng nguy c∆° ƒë·ªôt qu·ªµ.",
     "hypothesis": "Ng∆∞·ªùi cao huy·∫øt √°p c√≥ nguy c∆° ƒë·ªôt qu·ªµ cao h∆°n.", "label": 0},
    {"premise": "Kh√°ng sinh c·∫ßn d√πng ƒë·ªß li·ªáu tr√¨nh.",
     "hypothesis": "Kh√¥ng ƒë∆∞·ª£c ng·ª´ng kh√°ng sinh gi·ªØa ch·ª´ng.", "label": 0},
    {"premise": "Ti·ªÅn s·∫£n gi·∫≠t c√≥ tri·ªáu ch·ª©ng tƒÉng huy·∫øt √°p v√† protein ni·ªáu.",
     "hypothesis": "Ti·ªÅn s·∫£n gi·∫≠t g√¢y tƒÉng huy·∫øt √°p.", "label": 0},
    {"premise": "Rong kinh l√† t√¨nh tr·∫°ng ch·∫£y m√°u kinh nguy·ªát k√©o d√†i.",
     "hypothesis": "Rong kinh g√¢y m·∫•t m√°u k√©o d√†i.", "label": 0},
    
    # NEUTRAL examples
    {"premise": "Acid folic quan tr·ªçng trong thai k·ª≥.",
     "hypothesis": "U·ªëng 2 l√≠t n∆∞·ªõc m·ªói ng√†y.", "label": 1},
    {"premise": "Paracetamol l√† thu·ªëc gi·∫£m ƒëau.",
     "hypothesis": "Vitamin C c√≥ trong cam.", "label": 1},
    {"premise": "B·ªánh ti·ªÉu ƒë∆∞·ªùng c·∫ßn ki·ªÉm so√°t ƒë∆∞·ªùng huy·∫øt.",
     "hypothesis": "T·∫≠p th·ªÉ d·ª•c t·ªët cho tim m·∫°ch.", "label": 1},
    {"premise": "Thu·ªëc kh√°ng sinh c·∫ßn k√™ ƒë∆°n.",
     "hypothesis": "Ng·ªß ƒë·ªß 8 ti·∫øng m·ªói ng√†y.", "label": 1},
    {"premise": "Cao huy·∫øt √°p c·∫ßn u·ªëng thu·ªëc h·∫° √°p.",
     "hypothesis": "ƒÇn nhi·ªÅu rau xanh t·ªët cho s·ª©c kh·ªèe.", "label": 1},
    {"premise": "Vi√™m gan B l√† b·ªánh truy·ªÅn nhi·ªÖm.",
     "hypothesis": "Ung th∆∞ ph·ªïi li√™n quan ƒë·∫øn h√∫t thu·ªëc.", "label": 1},
    {"premise": "Ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng n√™n h·∫°n ch·∫ø ƒë∆∞·ªùng.",
     "hypothesis": "Canxi c·∫ßn cho x∆∞∆°ng ch·∫Øc kh·ªèe.", "label": 1},
    {"premise": "S·ªët l√† tri·ªáu ch·ª©ng c·ªßa nhi·ªÖm tr√πng.",
     "hypothesis": "ƒêau l∆∞ng c√≥ th·ªÉ do ng·ªìi sai t∆∞ th·∫ø.", "label": 1},
]

MEDICAL_REPEAT = 50
repeated_medical = medical_examples * MEDICAL_REPEAT
print(f"   Created {len(medical_examples)} unique examples, {len(repeated_medical)} after repeat")

# ============================================================
# Step 2: Load Datasets
# ============================================================
print("\nüì• Step 2: Loading ViANLI dataset...")

dataset = load_dataset("uitnlp/ViANLI")
print(f"   ViANLI: train={len(dataset['train'])}, val={len(dataset['validation'])}, test={len(dataset['test'])}")

def encode_labels(examples):
    label_map = {"entailment": 0, "neutral": 1, "contradiction": 2}
    examples["label"] = [label_map[label] for label in examples["label"]]
    return examples

dataset = dataset.map(encode_labels, batched=True)

medical_dataset = Dataset.from_list(repeated_medical)
combined_train = concatenate_datasets([dataset["train"], medical_dataset])
print(f"   Combined train: {len(combined_train)} samples")

# ============================================================
# Step 3: Load Model from Checkpoint
# ============================================================
print(f"\nüîß Step 3: Loading model from {CHECKPOINT_DIR}...")

tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)
model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"‚úÖ Model loaded! Device: {device}")

# ============================================================
# Step 4: Preprocess
# ============================================================
print("\n‚öôÔ∏è Step 4: Preprocessing...")

def preprocess(examples):
    return tokenizer(
        examples["premise"],
        examples["hypothesis"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length"
    )

columns_to_remove = ["premise", "hypothesis"]
if "uid" in combined_train.column_names:
    columns_to_remove.append("uid")

tokenized_train = combined_train.map(
    preprocess, batched=True, remove_columns=columns_to_remove
).rename_column("label", "labels")

tokenized_val = dataset["validation"].map(
    preprocess, batched=True, remove_columns=["uid", "premise", "hypothesis"]
).rename_column("label", "labels")

tokenized_test = dataset["test"].map(
    preprocess, batched=True, remove_columns=["uid", "premise", "hypothesis"]
).rename_column("label", "labels")

print(f"‚úÖ Train: {len(tokenized_train)}, Val: {len(tokenized_val)}, Test: {len(tokenized_test)}")

# ============================================================
# Step 5: Setup LLRD
# ============================================================
print("\n‚öôÔ∏è Step 5: Setting up LLRD...")

def get_optimizer_grouped_parameters(model, base_lr, layer_decay, weight_decay=0.01):
    opt_parameters = []
    no_decay = ["bias", "LayerNorm.weight"]
    
    if hasattr(model, 'deberta'):
        num_layers = model.config.num_hidden_layers
        encoder = model.deberta
    else:
        num_layers = model.config.num_hidden_layers
        encoder = model.base_model
    
    print(f"   Applying LLRD to {num_layers} layers with decay={layer_decay}")
    
    # Embeddings
    lr_embed = base_lr * (layer_decay ** (num_layers + 1))
    opt_parameters.append({
        "params": [p for n, p in encoder.embeddings.named_parameters() if not any(nd in n for nd in no_decay)],
        "lr": lr_embed, "weight_decay": weight_decay
    })
    opt_parameters.append({
        "params": [p for n, p in encoder.embeddings.named_parameters() if any(nd in n for nd in no_decay)],
        "lr": lr_embed, "weight_decay": 0.0
    })
    
    # Encoder layers
    for layer_i in range(num_layers):
        lr_layer = base_lr * (layer_decay ** (num_layers - layer_i))
        layer_module = encoder.encoder.layer[layer_i]
        
        opt_parameters.append({
            "params": [p for n, p in layer_module.named_parameters() if not any(nd in n for nd in no_decay)],
            "lr": lr_layer, "weight_decay": weight_decay
        })
        opt_parameters.append({
            "params": [p for n, p in layer_module.named_parameters() if any(nd in n for nd in no_decay)],
            "lr": lr_layer, "weight_decay": 0.0
        })
    
    # Classifier head
    opt_parameters.append({
        "params": [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)],
        "lr": base_lr, "weight_decay": weight_decay
    })
    opt_parameters.append({
        "params": [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)],
        "lr": base_lr, "weight_decay": 0.0
    })
    
    return [l for l in opt_parameters if len(l["params"]) > 0]

# ============================================================
# Step 6: Custom Trainer
# ============================================================

class WeightedLossTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights.to(self.args.device)
        
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

# ============================================================
# Step 7: Training
# ============================================================
print(f"\nüöÄ Step 7: Continuing training (+{ADDITIONAL_EPOCHS} epochs)...")
print(f"   Base LR: {BASE_LEARNING_RATE} (lower for continued training)")
print("=" * 90)

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=BASE_LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    num_train_epochs=ADDITIONAL_EPOCHS,
    weight_decay=0.01,
    warmup_ratio=0.05,  # Less warmup for continued training
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=50,
    bf16=torch.cuda.is_available(),
    report_to="none",
)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": accuracy_score(labels, predictions)}

optimizer_grouped_params = get_optimizer_grouped_parameters(
    model, BASE_LEARNING_RATE, LAYER_DECAY
)

trainer = WeightedLossTrainer(
    class_weights=CLASS_WEIGHTS,
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    optimizers=(
        torch.optim.AdamW(optimizer_grouped_params, lr=BASE_LEARNING_RATE),
        None
    )
)

trainer.train()

print("\n" + "=" * 90)
print("‚úÖ Training complete!")

# ============================================================
# Step 8: Evaluate
# ============================================================
print("\nüìä Step 8: Evaluating...")

test_results = trainer.evaluate(tokenized_test)
print(f"   Test Accuracy: {test_results['eval_accuracy']:.4f}")

predictions = trainer.predict(tokenized_test)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids

print("\nüìã Classification Report:")
print(classification_report(
    true_labels, pred_labels,
    target_names=["entailment", "neutral", "contradiction"]
))

# ============================================================
# Step 9: Save Model
# ============================================================
print(f"\nüíæ Step 9: Saving to {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# ============================================================
# Step 10: Test Negation Trap Cases
# ============================================================
print("\nüß™ Step 10: Testing negation trap cases...")

from sentence_transformers import CrossEncoder
finetuned_model = CrossEncoder(OUTPUT_DIR, device=device)

test_cases = [
    {"name": "üö® Medication Contraindication",
     "doc": "Thu·ªëc A kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho tr·∫ª em d∆∞·ªõi 12 tu·ªïi.",
     "claim": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho tr·∫ª em.",
     "expected": "contradiction"},
    {"name": "üö® Pregnancy Warning",
     "doc": "Kh√¥ng n√™n s·ª≠ d·ª•ng thu·ªëc n√†y trong thai k·ª≥.",
     "claim": "C√≥ th·ªÉ s·ª≠ d·ª•ng thu·ªëc n√†y trong thai k·ª≥.",
     "expected": "contradiction"},
    {"name": "üö® Dosage Negation",
     "doc": "Kh√¥ng ƒë∆∞·ª£c d√πng qu√° 4g paracetamol m·ªói ng√†y.",
     "claim": "C√≥ th·ªÉ d√πng 6g paracetamol m·ªói ng√†y.",
     "expected": "contradiction"},
    {"name": "Symptom Presence",
     "doc": "B·ªánh nh√¢n kh√¥ng c√≥ tri·ªáu ch·ª©ng s·ªët.",
     "claim": "B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng s·ªët.",
     "expected": "contradiction"},
    {"name": "Antibiotic + Alcohol",
     "doc": "Kh√¥ng u·ªëng r∆∞·ª£u khi ƒëang d√πng kh√°ng sinh.",
     "claim": "C√≥ th·ªÉ u·ªëng r∆∞·ª£u khi d√πng kh√°ng sinh.",
     "expected": "contradiction"},
    {"name": "‚úÖ Entailment (exact)",
     "doc": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho ng∆∞·ªùi l·ªõn.",
     "claim": "Thu·ªëc A ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh cho ng∆∞·ªùi l·ªõn.",
     "expected": "entailment"},
    {"name": "‚úÖ Entailment (inference)",
     "doc": "Acid folic quan tr·ªçng trong thai k·ª≥ v√¨ gi√∫p ngƒÉn d·ªã t·∫≠t ·ªëng th·∫ßn kinh.",
     "claim": "Acid folic c·∫ßn thi·∫øt cho thai nhi.",
     "expected": "entailment"},
    {"name": "‚ö™ Neutral (unrelated)",
     "doc": "Acid folic quan tr·ªçng trong thai k·ª≥.",
     "claim": "U·ªëng 2 l√≠t n∆∞·ªõc m·ªói ng√†y.",
     "expected": "neutral"},
    {"name": "‚ö™ Neutral (diff medical)",
     "doc": "Paracetamol l√† thu·ªëc gi·∫£m ƒëau.",
     "claim": "Vitamin C c√≥ trong cam.",
     "expected": "neutral"},
]

NLI_LABELS = ["entailment", "neutral", "contradiction"]
passed = 0

for test in test_cases:
    scores = finetuned_model.predict([(test['doc'], test['claim'])])[0]
    pred_idx = np.argmax(scores)
    predicted = NLI_LABELS[pred_idx]
    
    is_correct = predicted == test['expected']
    if is_correct:
        passed += 1
    
    status = "‚úÖ" if is_correct else "‚ùå"
    print(f"\n{status} {test['name']}")
    print(f"   Expected: {test['expected'].upper()}")
    print(f"   Predicted: {predicted.upper()}")
    print(f"   Scores: E={scores[0]:.2f}, N={scores[1]:.2f}, C={scores[2]:.2f}")

print("\n" + "=" * 90)
print(f"RESULTS: {passed}/{len(test_cases)} tests passed!")
print("=" * 90)
print(f"\nüì¶ Model saved at: {os.path.abspath(OUTPUT_DIR)}")
